{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 16:53:19.965244: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathan/workspace/neural_art_bots/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MacBook Pro Microphone', 'MacBook Pro Speakers', 'Microsoft Teams Audio', 'ZoomAudioDevice']\n",
      "should be the internal microphone:  MacBook Pro Microphone\n",
      "/Users/nathan/workspace/tensor_flow_projects/neural_art_bots/output_images/2023_02_21_16_\n"
     ]
    }
   ],
   "source": [
    "# for audio recognizion\n",
    "import pyaudio as audio\n",
    "import speech_recognition as sr\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import RegexpParser\n",
    "# for stable diffusion\n",
    "import os\n",
    "import random\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from math import ceil, floor\n",
    "\n",
    "# create a list of stopwords to ignore...\n",
    "stopwords = set(['shan', 'same', \"wasn't\", \"she's\", 'they', 'off', \n",
    "                 \"needn't\", \"weren't\", 'as', 'some', 'and', 'from', 'other', \n",
    "                 \"shouldn't\", \"shan't\", 'to', 'does', 'was', 'has', 'so', 'himself', \n",
    "                 'do', 'below', \"doesn't\", \"that'll\", 'its', 'these', 'are', 'more', \n",
    "                 'aren', 'all', 'whom', 'shouldn', 'too', 'over', \"you've\", 'him', 'o', \n",
    "                 'his', 'be', \"you'll\", 'out', 'against', 'most', 'if', 'hasn', 'own', \n",
    "                 's', 'what', 'theirs', 'or', \"it's\", 'will', \"don't\", 'is', 'been', \n",
    "                 'who', 'yourselves', 'her', 'did', 'the', 'up', 'there', 'ourselves', \n",
    "                 'during', 'mightn', \"you'd\", 'further', 'very', 'those', 'for', 'but', \n",
    "                 'an', 'in', 'nor', \"mightn't\", 've', 'both', 'until', 'isn', 'ain', \n",
    "                 \"didn't\", 'than', 'themselves', 'myself', \"couldn't\", 'now', 'herself', \n",
    "                 'any', 'by', \"wouldn't\", 'about', 'after', 'here', 'doesn', 'a', 'which', \n",
    "                 'd', 'y', 'were', 'couldn', \"aren't\", 'i', 'then', 'being', 'just', 'our', \n",
    "                 \"haven't\", 't', 'wouldn', 're', \"mustn't\", 'while', 'with', 'only', 'under', \n",
    "                 'ma', 'again', 'can', 'ours', 'through', \"hadn't\", 'when', 'hers', \"isn't\", \n",
    "                 'of', 'few', 'my', 'had', 'before', 'where', 'wasn', \"should've\", 'she', \n",
    "                 'your', 'haven', 'weren', 'on', 'have', 'he', 'between', 'me', 'down', \n",
    "                 'should', 'mustn', 'their', 'am', 'above', 'll', 'such', 'why', 'no', \n",
    "                 'you', 'it', 'because', 'into', 'm', \"you're\", 'that', 'itself', 'not', \n",
    "                 'hadn', \"won't\", 'we', 'don', 'doing', 'won', 'them', 'this', \"hasn't\", \n",
    "                 'how', 'at', 'needn', 'once', 'having', 'yours', 'each', 'yourself', 'didn'])\n",
    "# create stemmer object to reduce words to their root forms...\n",
    "stemmer = PorterStemmer()\n",
    "# create a queue of the last 100 words identified by the program\n",
    "recent_text_q = []\n",
    "# activate macbook microphone stream\n",
    "## figure out what audio device our microphone is\n",
    "print(sr.Microphone.list_microphone_names())\n",
    "index = 0\n",
    "print(\"should be the internal microphone: \", sr.Microphone.list_microphone_names()[index])\n",
    "# create a microphone object\n",
    "internal_mic = sr.Microphone(device_index = index)\n",
    "# create a speech recognition object\n",
    "recognizer = sr.Recognizer()\n",
    "return_str = \"\"\n",
    "\n",
    "# create a basic naming root for any files that we create and want to save during this session\n",
    "time_str = datetime.now().strftime(\"%Y_%m_%d_%H_\")\n",
    "output_name = os.getcwd() + \"/output_images/\" + time_str\n",
    "img_num = 0\n",
    "print(output_name)\n",
    "\n",
    "words = []\n",
    "nouns = []\n",
    "verbs = []\n",
    "adjectives = []\n",
    "determiners = []\n",
    "recent_text_q = []\n",
    "\n",
    "def addSimplePhraise():\n",
    "    # generate multiple phraises, then determine best one using nltk\n",
    "    return \"{} {} {} {} {}\".format(randomNoun(), randomAdj(), randomNoun(), randomVerb(), randomNoun())\n",
    "\n",
    "def randomNoun():\n",
    "    if len(nouns):\n",
    "        return nouns[random.randint(0, len(nouns) - 1)]\n",
    "    return \"\"\n",
    "\n",
    "def randomVerb():\n",
    "    if len(verbs):\n",
    "        return verbs[random.randint(0, len(verbs) - 1)]\n",
    "    return \"\"\n",
    "\n",
    "def randomAdj():\n",
    "    if len(adjectives):\n",
    "        return adjectives[random.randint(0, len(adjectives) - 1)]\n",
    "    return \"\"\n",
    "\n",
    "def randomDeterminer():\n",
    "    if len(determiners):\n",
    "        return determiners[random.randint(0, len(determiners))]\n",
    "    return \"\"\n",
    "\n",
    "def fifoIn(lst, val, max_len):\n",
    "    # check if item needs to be removed\n",
    "    if len(lst) >= max_len:\n",
    "        lst.pop(0)\n",
    "    lst.append(val)\n",
    "    return lst\n",
    "\n",
    "def getStrFromTuple(lst):\n",
    "    r = \"\"\n",
    "    for l in lst:\n",
    "        r.join(l[0]).join(\" \")\n",
    "    return r\n",
    "\n",
    "def getStrFromList(lst):\n",
    "    print(lst)\n",
    "    r = \"\"\n",
    "    for i in range(len(lst)):\n",
    "        print(i)\n",
    "        r.join(lst[i]).join(\" \")\n",
    "        print(r)\n",
    "    return r\n",
    "\n",
    "def generatePromptString():\n",
    "    return \"{}, {}, {}\".format(addSimplePhraise(), addSimplePhraise(), addSimplePhraise())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "only_unique_words = True\n",
    "max_q_len = 100\n",
    "###############33\n",
    "\n",
    "gender = 'm' # as oppose to  'f'\n",
    "vnum = str(random.randint(1, 5))\n",
    "voice = '{}{}'.format(gender, vnum)\n",
    "pitch = str(random.randint(10, 90))\n",
    "\n",
    "batch = 2\n",
    "seed = None\n",
    "steps = 15\n",
    "\n",
    "nprompt_string = \"dogs, cars, people\"\n",
    "\n",
    "# 0.7 is the default\n",
    "# Higher guidance scale encourages to generate \n",
    "# images that are closely linked to the text prompt, \n",
    "# usually at the expense of lower image quality.\n",
    "uncon_guide_scale = 0.3\n",
    "\n",
    "# model = keras_cv.models.StableDiffusion(img_width=512, img_height=512)\n",
    "# MQ model for standard operation speed\n",
    "model = keras_cv.models.StableDiffusion(img_width=640, img_height=480, jit_compile=True)\n",
    "# LQ and fast version of the model\n",
    "# model = keras_cv.models.StableDiffusion(img_width=96, img_height=96, jit_compile=False)\n",
    "# HQ and slow version\n",
    "#model = keras_cv.models.StableDiffusion(img_width=512, img_height=512, jit_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listenForWords(recent_text_q, words, nouns, verbs, adjectives, determiners):\n",
    "    # capture audio from the microphone\n",
    "    with internal_mic as source:\n",
    "        # adjust for background noise to increase success rate\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        # identify any spoken words in the audio\n",
    "        print(\"Speech Recognizer Enabled\");\n",
    "        audio = recognizer.listen(source)\n",
    "        print(\"audio exported from source\")\n",
    "        raw_string = \"the the \"\n",
    "        try:\n",
    "            raw_string = recognizer.recognize_google(audio)\n",
    "        except:\n",
    "            print(\" .\")\n",
    "\n",
    "        print(\"{} of tokenized words returned from google: {}\".format(type(raw_string), raw_string))\n",
    "        # remove words from string\n",
    "\n",
    "        return_str = raw_string.split()\n",
    "        wn = len(return_str)\n",
    "        # remove any words in the stopwords\n",
    "                # append spoken words to the running FIFO of all words\n",
    "            # if append to buffer according to type of grammer\n",
    "        words = [i for i in return_str if i not in stopwords]\n",
    "        for word in words:\n",
    "            recent_text_q = fifoIn(recent_text_q, word, max_q_len)\n",
    "        print(\"{} words removed from stopwords\".format(wn - len(words)))\n",
    "        if only_unique_words is True:\n",
    "            words = [i for i in return_str if i not in recent_text_q]\n",
    "            print(\"{} words removed from priorwords\".format(wn - len(words)))\n",
    "        # append the word type to the word string, returnig a tuple (str, type)\n",
    "        words = pos_tag(words)\n",
    "        # reconstruct a string to be sent to a stable diffusion network\n",
    "        print(\"{} word tags tuples : {}\".format(len(words), words))\n",
    "        # TODO - BUG - because we are not operating on the \"words\" object we are running into an issue\n",
    "        for word in words:\n",
    "            print(\"word : {}\".format(word))\n",
    "            if word[1].startswith(\"NN\"):\n",
    "                nouns = fifoIn(nouns, word[0], max_q_len)\n",
    "            elif word[1].startswith(\"VB\"):\n",
    "                verbs = fifoIn(verbs, word[0], max_q_len)\n",
    "            elif word[1].startswith(\"JJ\"):\n",
    "                adjectives = fifoIn(adjectives, word[0], max_q_len)\n",
    "            elif word[1].startswith(\"DT\"):\n",
    "                determiners = fifoIn(determiners, word[0], max_q_len)\n",
    "            else:\n",
    "                print(\"word not categorized: {}\".format(word))\n",
    "            prompt_string = \"\"\n",
    "        print(\"{} nouns are saved: {}\".format(len(nouns), nouns))\n",
    "        print(\"{} verbs are saved: {}\".format(len(verbs), verbs))\n",
    "        print(\"{} adjectives are saved: {}\".format(len(adjectives), adjectives))\n",
    "        print(\"{} determiners are saved: {}\".format(len(determiners), determiners))\n",
    "        # create a chunk grammar statement\n",
    "        \"\"\"\n",
    "            According to the rule you created, your chunks:\n",
    "            Start with an optional (?) determiner ('DT')\n",
    "            Can have any number (*) of adjectives (JJ)\n",
    "            End with a noun (<NN>)\n",
    "        grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "        chunk_parser = RegexpParser(grammar)\n",
    "        tree = chunk_parser.parse(words_tags)\n",
    "        \"\"\"\n",
    "        print(\"{} identified words: \".format(len(recent_text_q)),\n",
    "              recent_text_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Recognizer Enabled\n",
      "audio exported from source\n",
      " .\n",
      "<class 'str'> of tokenized words returned from google: the the \n",
      "2 words removed from stopwords\n",
      "0 words removed from priorwords\n",
      "2 word tags tuples : [('the', 'DT'), ('the', 'DT')]\n",
      "word : ('the', 'DT')\n",
      "word : ('the', 'DT')\n",
      "0 nouns are saved: []\n",
      "0 verbs are saved: []\n",
      "0 adjectives are saved: []\n",
      "2 determiners are saved: ['the', 'the']\n",
      "0 identified words:  []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "listenForWords(recent_text_q, words, nouns, verbs, adjectives, determiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ,     ,     \n"
     ]
    }
   ],
   "source": [
    "prompt_string = generatePromptString()\n",
    "print(prompt_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Speech using espeak in OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using espeak to say the following command     ,     ,     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = 'espeak -s 180 -v {} -p {} \"{}\"\\n'.format(voice, pitch, prompt_string)\n",
    "print(\"using espeak to say the following command {}\".format(prompt_string))\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 16:53:46.468584: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 16:53:56.390912: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f8b5ae9a910 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-02-21 16:53:56.390945: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Host, Default Version\n",
      "2023-02-21 16:53:56.392748: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 16:53:56.622708: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-02-21 16:54:04.251951: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nathan/workspace/neural_art_bots/venv/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "16/16 [==============================] - 817s 46s/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m images \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtext_to_image(\n\u001b[1;32m      3\u001b[0m     prompt_string,\n\u001b[1;32m      4\u001b[0m     negative_prompt\u001b[39m=\u001b[39mnprompt_string,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     unconditional_guidance_scale\u001b[39m=\u001b[39muncon_guide_scale,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m keras\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mclear_session()  \u001b[39m# Clear session to preserve memory\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m plot_images(images)\n\u001b[1;32m     13\u001b[0m img_num \u001b[39m=\u001b[39m save_images(images)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_images' is not defined"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "images = model.text_to_image(\n",
    "    prompt_string,\n",
    "    negative_prompt=nprompt_string,\n",
    "    seed=seed,\n",
    "    num_steps=steps,\n",
    "    batch_size=batch,\n",
    "    unconditional_guidance_scale=uncon_guide_scale,\n",
    ")\n",
    "\n",
    "keras.backend.clear_session()  # Clear session to preserve memory\n",
    "plot_images(images)\n",
    "img_num = save_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cool, lets now put all the chunks together =)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84f100e172d30523e09c9ea3b146d7b4e44e98c1b89071d9f6625876d1cbe325"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
