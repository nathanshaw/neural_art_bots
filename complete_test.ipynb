{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MacBook Pro Microphone', 'MacBook Pro Speakers', 'Microsoft Teams Audio', 'ZoomAudioDevice']\n",
      "should be the internal microphone:  MacBook Pro Microphone\n",
      "/Users/nathan/workspace/neural_art_bots/output_images/2023_02_19_17_\n"
     ]
    }
   ],
   "source": [
    "# for audio recognizion\n",
    "import pyaudio as audio\n",
    "import speech_recognition as sr\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import RegexpParser\n",
    "# for stable diffusion\n",
    "import os\n",
    "import random\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from math import ceil, floor\n",
    "\n",
    "# create a list of stopwords to ignore...\n",
    "stopwords = set(['shan', 'same', \"wasn't\", \"she's\", 'they', 'off', \n",
    "                 \"needn't\", \"weren't\", 'as', 'some', 'and', 'from', 'other', \n",
    "                 \"shouldn't\", \"shan't\", 'to', 'does', 'was', 'has', 'so', 'himself', \n",
    "                 'do', 'below', \"doesn't\", \"that'll\", 'its', 'these', 'are', 'more', \n",
    "                 'aren', 'all', 'whom', 'shouldn', 'too', 'over', \"you've\", 'him', 'o', \n",
    "                 'his', 'be', \"you'll\", 'out', 'against', 'most', 'if', 'hasn', 'own', \n",
    "                 's', 'what', 'theirs', 'or', \"it's\", 'will', \"don't\", 'is', 'been', \n",
    "                 'who', 'yourselves', 'her', 'did', 'the', 'up', 'there', 'ourselves', \n",
    "                 'during', 'mightn', \"you'd\", 'further', 'very', 'those', 'for', 'but', \n",
    "                 'an', 'in', 'nor', \"mightn't\", 've', 'both', 'until', 'isn', 'ain', \n",
    "                 \"didn't\", 'than', 'themselves', 'myself', \"couldn't\", 'now', 'herself', \n",
    "                 'any', 'by', \"wouldn't\", 'about', 'after', 'here', 'doesn', 'a', 'which', \n",
    "                 'd', 'y', 'were', 'couldn', \"aren't\", 'i', 'then', 'being', 'just', 'our', \n",
    "                 \"haven't\", 't', 'wouldn', 're', \"mustn't\", 'while', 'with', 'only', 'under', \n",
    "                 'ma', 'again', 'can', 'ours', 'through', \"hadn't\", 'when', 'hers', \"isn't\", \n",
    "                 'of', 'few', 'my', 'had', 'before', 'where', 'wasn', \"should've\", 'she', \n",
    "                 'your', 'haven', 'weren', 'on', 'have', 'he', 'between', 'me', 'down', \n",
    "                 'should', 'mustn', 'their', 'am', 'above', 'll', 'such', 'why', 'no', \n",
    "                 'you', 'it', 'because', 'into', 'm', \"you're\", 'that', 'itself', 'not', \n",
    "                 'hadn', \"won't\", 'we', 'don', 'doing', 'won', 'them', 'this', \"hasn't\", \n",
    "                 'how', 'at', 'needn', 'once', 'having', 'yours', 'each', 'yourself', 'didn'])\n",
    "# create stemmer object to reduce words to their root forms...\n",
    "stemmer = PorterStemmer()\n",
    "# create a queue of the last 100 words identified by the program\n",
    "recent_text_q = []\n",
    "# activate macbook microphone stream\n",
    "## figure out what audio device our microphone is\n",
    "print(sr.Microphone.list_microphone_names())\n",
    "index = 0\n",
    "print(\"should be the internal microphone: \", sr.Microphone.list_microphone_names()[index])\n",
    "# create a microphone object\n",
    "internal_mic = sr.Microphone(device_index = index)\n",
    "# create a speech recognition object\n",
    "recognizer = sr.Recognizer()\n",
    "return_str = \"\"\n",
    "\n",
    "# create a basic naming root for any files that we create and want to save during this session\n",
    "time_str = datetime.now().strftime(\"%Y_%m_%d_%H_\")\n",
    "output_name = os.getcwd() + \"/output_images/\" + time_str\n",
    "img_num = 0\n",
    "print(output_name)\n",
    "\n",
    "words = []\n",
    "nouns = []\n",
    "verbs = []\n",
    "adjectives = []\n",
    "determiners = []\n",
    "recent_text_q = []\n",
    "\n",
    "def addSimplePhraise():\n",
    "    # generate multiple phraises, then determine best one using nltk\n",
    "    return \"{} {} {} {} {}\".format(randomNoun(), randomAdj(), randomNoun(), randomVerb(), randomNoun())\n",
    "\n",
    "def randomNoun():\n",
    "    if len(nouns):\n",
    "        return nouns[random.randint(0, len(nouns) - 1)]\n",
    "    return \"\"\n",
    "\n",
    "def randomVerb():\n",
    "    if len(verbs):\n",
    "        return verbs[random.randint(0, len(verbs) - 1)]\n",
    "    return \"\"\n",
    "\n",
    "def randomAdj():\n",
    "    if len(adjectives):\n",
    "        return adjectives[random.randint(0, len(adjectives) - 1)]\n",
    "    return \"\"\n",
    "\n",
    "def randomDeterminer():\n",
    "    if len(determiners):\n",
    "        return determiners[random.randint(0, len(determiners))]\n",
    "    return \"\"\n",
    "\n",
    "def fifoIn(lst, val, max_len):\n",
    "    # check if item needs to be removed\n",
    "    if len(lst) >= max_len:\n",
    "        lst.pop(0)\n",
    "    lst.append(val)\n",
    "    return lst\n",
    "\n",
    "def getStrFromTuple(lst):\n",
    "    r = \"\"\n",
    "    for l in lst:\n",
    "        r.join(l[0]).join(\" \")\n",
    "    return r\n",
    "\n",
    "def getStrFromList(lst):\n",
    "    print(lst)\n",
    "    r = \"\"\n",
    "    for i in range(len(lst)):\n",
    "        print(i)\n",
    "        r.join(lst[i]).join(\" \")\n",
    "        print(r)\n",
    "    return r\n",
    "\n",
    "def generatePromptString():\n",
    "    return \"{}, {}, {}\".format(addSimplePhraise(), addSimplePhraise(), addSimplePhraise())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "only_unique_words = True\n",
    "max_q_len = 100\n",
    "###############33\n",
    "\n",
    "gender = 'm' # as oppose to  'f'\n",
    "vnum = str(random.randint(1, 5))\n",
    "voice = '{}{}'.format(gender, vnum)\n",
    "pitch = str(random.randint(10, 90))\n",
    "\n",
    "batch = 2\n",
    "seed = None\n",
    "steps = 15\n",
    "\n",
    "nprompt_string = \"dogs, cars, people\"\n",
    "\n",
    "# 0.7 is the default\n",
    "# Higher guidance scale encourages to generate \n",
    "# images that are closely linked to the text prompt, \n",
    "# usually at the expense of lower image quality.\n",
    "uncon_guide_scale = 0.3\n",
    "\n",
    "# model = keras_cv.models.StableDiffusion(img_width=512, img_height=512)\n",
    "# MQ model for standard operation speed\n",
    "model = keras_cv.models.StableDiffusion(img_width=640, img_height=480, jit_compile=True)\n",
    "# LQ and fast version of the model\n",
    "# model = keras_cv.models.StableDiffusion(img_width=96, img_height=96, jit_compile=False)\n",
    "# HQ and slow version\n",
    "#model = keras_cv.models.StableDiffusion(img_width=512, img_height=512, jit_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listenForWords(recent_text_q, words, nouns, verbs, adjectives, determiners):\n",
    "    # capture audio from the microphone\n",
    "    with internal_mic as source:\n",
    "        # adjust for background noise to increase success rate\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        # identify any spoken words in the audio\n",
    "        print(\"Speech Recognizer Enabled\");\n",
    "        audio = recognizer.listen(source)\n",
    "        print(\"audio exported from source\")\n",
    "        raw_string = \"the the \"\n",
    "        try:\n",
    "            raw_string = recognizer.recognize_google(audio)\n",
    "        except:\n",
    "            print(\" .\")\n",
    "\n",
    "        print(\"{} of tokenized words returned from google: {}\".format(type(raw_string), raw_string))\n",
    "        # remove words from string\n",
    "\n",
    "        return_str = raw_string.split()\n",
    "        wn = len(return_str)\n",
    "        # remove any words in the stopwords\n",
    "                # append spoken words to the running FIFO of all words\n",
    "            # if append to buffer according to type of grammer\n",
    "        words = [i for i in return_str if i not in stopwords]\n",
    "        for word in words:\n",
    "            recent_text_q = fifoIn(recent_text_q, word, max_q_len)\n",
    "        print(\"{} words removed from stopwords\".format(wn - len(words)))\n",
    "        if only_unique_words is True:\n",
    "            words = [i for i in return_str if i not in recent_text_q]\n",
    "            print(\"{} words removed from priorwords\".format(wn - len(words)))\n",
    "        # append the word type to the word string, returnig a tuple (str, type)\n",
    "        words = pos_tag(words)\n",
    "        # reconstruct a string to be sent to a stable diffusion network\n",
    "        print(\"{} word tags tuples : {}\".format(len(words), words))\n",
    "        # TODO - BUG - because we are not operating on the \"words\" object we are running into an issue\n",
    "        for word in words:\n",
    "            print(\"word : {}\".format(word))\n",
    "            if word[1].startswith(\"NN\"):\n",
    "                nouns = fifoIn(nouns, word[0], max_q_len)\n",
    "            elif word[1].startswith(\"VB\"):\n",
    "                verbs = fifoIn(verbs, word[0], max_q_len)\n",
    "            elif word[1].startswith(\"JJ\"):\n",
    "                adjectives = fifoIn(adjectives, word[0], max_q_len)\n",
    "            elif word[1].startswith(\"DT\"):\n",
    "                determiners = fifoIn(determiners, word[0], max_q_len)\n",
    "            else:\n",
    "                print(\"word not categorized: {}\".format(word))\n",
    "            prompt_string = \"\"\n",
    "        print(\"{} nouns are saved: {}\".format(len(nouns), nouns))\n",
    "        print(\"{} verbs are saved: {}\".format(len(verbs), verbs))\n",
    "        print(\"{} adjectives are saved: {}\".format(len(adjectives), adjectives))\n",
    "        print(\"{} determiners are saved: {}\".format(len(determiners), determiners))\n",
    "        # create a chunk grammar statement\n",
    "        \"\"\"\n",
    "            According to the rule you created, your chunks:\n",
    "            Start with an optional (?) determiner ('DT')\n",
    "            Can have any number (*) of adjectives (JJ)\n",
    "            End with a noun (<NN>)\n",
    "        grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "        chunk_parser = RegexpParser(grammar)\n",
    "        tree = chunk_parser.parse(words_tags)\n",
    "        \"\"\"\n",
    "        print(\"{} identified words: \".format(len(recent_text_q)),\n",
    "              recent_text_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Recognizer Enabled\n",
      "audio exported from source\n",
      "result2:\n",
      "{   'alternative': [   {   'confidence': 0.94409424,\n",
      "                           'transcript': \"I don't really understand if no \"\n",
      "                                         'words are identified then whether '\n",
      "                                         'they show up in the list'},\n",
      "                       {   'transcript': \"I don't really understand if no \"\n",
      "                                         'words are identified then why do '\n",
      "                                         'they show up in the list'},\n",
      "                       {   'transcript': \"I don't really understand if \"\n",
      "                                         \"nobody's are identified then whether \"\n",
      "                                         'they show up in the list'},\n",
      "                       {   'transcript': \"I don't really understand if no \"\n",
      "                                         'words are identified then whether '\n",
      "                                         'they show up in the list they'},\n",
      "                       {   'transcript': \"I don't really understand if no \"\n",
      "                                         'words are identified then why do '\n",
      "                                         'they show up in the list they'}],\n",
      "    'final': True}\n",
      "<class 'str'> of tokenized words returned from google: I don't really understand if no words are identified then whether they show up in the list\n",
      "9 words removed from stopwords\n",
      "8 words removed from priorwords\n",
      "9 word tags tuples : [(\"don't\", 'NN'), ('if', 'IN'), ('no', 'DT'), ('are', 'VBP'), ('then', 'RB'), ('they', 'PRP'), ('up', 'RP'), ('in', 'IN'), ('the', 'DT')]\n",
      "word : (\"don't\", 'NN')\n",
      "word : ('if', 'IN')\n",
      "word not categorized: ('if', 'IN')\n",
      "word : ('no', 'DT')\n",
      "word : ('are', 'VBP')\n",
      "word : ('then', 'RB')\n",
      "word not categorized: ('then', 'RB')\n",
      "word : ('they', 'PRP')\n",
      "word not categorized: ('they', 'PRP')\n",
      "word : ('up', 'RP')\n",
      "word not categorized: ('up', 'RP')\n",
      "word : ('in', 'IN')\n",
      "word not categorized: ('in', 'IN')\n",
      "word : ('the', 'DT')\n",
      "1 nouns are saved: [\"don't\"]\n",
      "1 verbs are saved: ['are']\n",
      "0 adjectives are saved: []\n",
      "4 determiners are saved: ['the', 'the', 'no', 'the']\n",
      "19 identified words:  ['listening', 'Words', 'man', 'hey', 'man', \"let's\", 'get', 'words', 'man', 'yo', 'dude', 'I', 'really', 'understand', 'words', 'identified', 'whether', 'show', 'list']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "listenForWords(recent_text_q, words, nouns, verbs, adjectives, determiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "don't  don't are don't, don't  don't are don't, don't  don't are don't\n"
     ]
    }
   ],
   "source": [
    "prompt_string = generatePromptString()\n",
    "print(prompt_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Speech using espeak in OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using espeak to say the following command don't  don't are don't, don't  don't are don't, don't  don't are don't\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = 'espeak -s 180 -v {} -p {} \"{}\"\\n'.format(voice, pitch, prompt_string)\n",
    "print(\"using espeak to say the following command {}\".format(prompt_string))\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 18:02:05.631753: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-19 18:02:12.251524: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fa425a9b240 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-02-19 18:02:12.251580: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Host, Default Version\n",
      "2023-02-19 18:02:12.251881: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-19 18:02:12.380553: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-02-19 18:02:15.973362: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nathan/workspace/neural_art_bots/venv/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      " 4/16 [======>.......................] - ETA: 6:15"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "images = model.text_to_image(\n",
    "    prompt_string,\n",
    "    negative_prompt=nprompt_string,\n",
    "    seed=seed,\n",
    "    num_steps=steps,\n",
    "    batch_size=batch,\n",
    "    unconditional_guidance_scale=uncon_guide_scale,\n",
    ")\n",
    "\n",
    "keras.backend.clear_session()  # Clear session to preserve memory\n",
    "plot_images(images)\n",
    "img_num = save_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cool, lets now put all the chunks together =)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84f100e172d30523e09c9ea3b146d7b4e44e98c1b89071d9f6625876d1cbe325"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
